{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model\nfrom sklearn.model_selection import KFold\n# from itertools import combinations\nfrom sklearn.feature_selection import SelectKBest, f_classif\n!pip install meteostat\nfrom meteostat import Point, Daily\nimport missingno as msno\nimport calendar\nfrom datetime import datetime\n%load_ext memory_profiler","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:37:36.716127Z","iopub.execute_input":"2021-11-08T02:37:36.716428Z","iopub.status.idle":"2021-11-08T02:37:53.135322Z","shell.execute_reply.started":"2021-11-08T02:37:36.716397Z","shell.execute_reply":"2021-11-08T02:37:53.134233Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from meteostat import Hourly\n# import warnings\n# warnings.simplefilter('ignore')\nimport gc\nimport subprocess\nimport dask\nimport dask.dataframe as dd","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:37:53.137199Z","iopub.execute_input":"2021-11-08T02:37:53.137500Z","iopub.status.idle":"2021-11-08T02:37:53.811391Z","shell.execute_reply.started":"2021-11-08T02:37:53.137467Z","shell.execute_reply":"2021-11-08T02:37:53.810346Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"In the function below we'll import our data. Given the size of the data we're going to use a bit of a different approach than regular 'pd.read_csv':\n\n* the sheers size of 1 file prevents us from downloading all the 12 files and merging them into one dataframe - we will have to sample 10% of each file, and merge all the sample into 1 massive dataframe\n    * the 10 % is just a number we'll set for now, but our imports function will have an option to change that at the input stage\n* reading only 1 file is already a big task - we'll split that process into 'chunks' and merge all chunks together after all of them are ready\n* every file represents 1 month of trips data, sort of... unfortunatelly it's common to come across dates that are out of range for their files, we can find some unnaturally long trips etc.\n    * this creates our first problem: we will have to do some basic data cleaning (steps 1-3) before we'll start sampling the dataframe, this will prevent us from sampling incorrect/ dirty/ null data, the tradeof: we're going to work on a full size file, which uses a lot of memory\n* we'll try to downcast numeric columns whenever possible to save memory\n\nAll of the above steps are going to be conducted inside a function - to prevent memory leaks. This method creates a new scope for the intermediate variables and removes them automatically when the interpreter exits the function.\n","metadata":{}},{"cell_type":"code","source":"months = []\n# we'll import the data inside a function to save memory:\ndef imports2(year,months_number,sample_part):\n    df_list = []\n    for el in list((range(1,months_number+1))):\n        month = str(el).zfill(2)\n        link = 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_'+str(year)+'-'+month+'.csv'\n        chunks = pd.read_csv(link, dtype={'tolls_amount': 'float64', 'RatecodeID':'float64',\n                                                           'trip_distance':'float64','store_and_fwd_flag':'category'}, chunksize=40000)\n        df = pd.concat(chunks, ignore_index=True)\n        del chunks\n        gc.collect()\n        # 1. remove nulls and insanely long trips:\n        df = df[~df['payment_type'].isnull()]\n        df = df[df['trip_distance']<500]\n        # 2. create a pickup date column, modify column dtypes:\n        df['pickup_date'] = df['tpep_pickup_datetime'].str[:11]\n        df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n        df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n        df['pickup_date'] = pd.to_datetime(df['pickup_date'])\n        # 3. lets make sure that the dataframe has only this months days:\n        df = df[(df['pickup_date']>=str(year)+'-'+month+'-01') & (df['pickup_date']<=str(year)+'-'+month+'-'+str(calendar.monthrange(year, el)[1]))].copy()\n        # 4. now lets sample only 20% of the original dataset:\n        df = df.sample(int(len(df)*sample_part))\n        # 5. change a few columns to integers to save memory:\n        integerize = ['passenger_count','VendorID','RatecodeID', 'payment_type']\n        for col in integerize:\n            df[col] = df[col].astype(int)\n            df[col] = pd.to_numeric(df[col], downcast=\"unsigned\")\n        # 6. new trick:\n        df_list.append(df)\n        del df\n        gc.collect()\n        if len(df_list)>1:\n            df_both = pd.concat([df_list[0],df_list[1]], axis=0)          \n            del df_list[0]\n            del df_list[0]            \n            gc.collect()\n            df_list.append(df_both)\n            del df_both\n            gc.collect()\n    return df_list[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:37:53.813201Z","iopub.execute_input":"2021-11-08T02:37:53.813537Z","iopub.status.idle":"2021-11-08T02:37:53.830568Z","shell.execute_reply.started":"2021-11-08T02:37:53.813492Z","shell.execute_reply":"2021-11-08T02:37:53.829660Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_2018 = imports2(2017,12,0.1)\ndf_2019 = imports2(2018,12,0.1)\ndf = pd.concat([df_2018,df_2019], axis=0)  \ndel df_2018\ndel df_2019\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:37:53.833726Z","iopub.execute_input":"2021-11-08T02:37:53.834169Z","iopub.status.idle":"2021-11-08T03:15:43.994897Z","shell.execute_reply.started":"2021-11-08T02:37:53.834111Z","shell.execute_reply":"2021-11-08T03:15:43.993457Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:37:06.615224Z","iopub.status.idle":"2021-11-08T02:37:06.615642Z","shell.execute_reply.started":"2021-11-08T02:37:06.615475Z","shell.execute_reply":"2021-11-08T02:37:06.615492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('PULocationID')['passenger_count'].std()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:15:48.916530Z","iopub.execute_input":"2021-11-08T02:15:48.916818Z","iopub.status.idle":"2021-11-08T02:15:49.275629Z","shell.execute_reply.started":"2021-11-08T02:15:48.916790Z","shell.execute_reply":"2021-11-08T02:15:49.274845Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:15:49.276815Z","iopub.execute_input":"2021-11-08T02:15:49.277047Z","iopub.status.idle":"2021-11-08T02:15:49.303805Z","shell.execute_reply.started":"2021-11-08T02:15:49.277020Z","shell.execute_reply":"2021-11-08T02:15:49.303243Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def feature_eng(df):\n    # trip length\n    df['trip_length'] = df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n    df['trip_length'] = df['trip_length'].astype('timedelta64[m]')\n\n    # day of the week\n    df['day_of_week'] = df['pickup_date'].dt.day_name()\n    df['day_of_week'] = df['day_of_week'].astype('category')\n    df['day_of_week'] = df['day_of_week'].cat.codes\n\n    # time of day\n    df['time_of_day'] = df['tpep_pickup_datetime'].dt.hour\n    df['time_of_day'] = df['time_of_day'].astype('category')\n    df['time_of_day'] = df['time_of_day'].cat.codes\n    return df\ndf = feature_eng(df)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:15:43.996838Z","iopub.execute_input":"2021-11-08T03:15:43.997216Z","iopub.status.idle":"2021-11-08T03:16:00.380799Z","shell.execute_reply.started":"2021-11-08T03:15:43.997158Z","shell.execute_reply":"2021-11-08T03:16:00.379720Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# df.to_csv('myfirst_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:16:02.800562Z","iopub.execute_input":"2021-11-08T02:16:02.800796Z","iopub.status.idle":"2021-11-08T02:25:06.885955Z","shell.execute_reply.started":"2021-11-08T02:16:02.800770Z","shell.execute_reply":"2021-11-08T02:25:06.884984Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(16,8))\n# plt.scatter(df.groupby(df['pickup_date'])['passenger_count'].sum().index,df.groupby(df['pickup_date'])['passenger_count'].sum().values )\n# ax.tick_params(axis = 'x',labelsize=12, rotation=45)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T02:25:06.889917Z","iopub.execute_input":"2021-11-08T02:25:06.890302Z","iopub.status.idle":"2021-11-08T02:25:08.051823Z","shell.execute_reply.started":"2021-11-08T02:25:06.890266Z","shell.execute_reply":"2021-11-08T02:25:08.050957Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Weather \nlets try importing weather data ...er from somewhere","metadata":{}},{"cell_type":"code","source":"start = df['pickup_date'].min()\nend = df['pickup_date'].max()\n\ndef get_weather(start, end):\n    # Create Point for NYC\n    location = Point(40.785091,-73.968285)\n\n    # Get daily data for 2018\n    data = Daily(location, start, end)\n    data = data.fetch()\n    data = data.drop(columns=['wpgt','tsun'])\n    data['time'] = data.index\n    data = data.rename(columns={'time':'pickup_date'})\n    return data\ndata = get_weather(start, end)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:00.382536Z","iopub.execute_input":"2021-11-08T03:16:00.383141Z","iopub.status.idle":"2021-11-08T03:16:09.560985Z","shell.execute_reply.started":"2021-11-08T03:16:00.383089Z","shell.execute_reply":"2021-11-08T03:16:09.559694Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def data_merge(data,df):\n    df2=df.merge(data, on='pickup_date', how='left') \n    df2.head()\n    del df\n    del data\n    gc.collect()\n    return df2\ndf2 = data_merge(data,df)\ndel data\ndel df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:09.563207Z","iopub.execute_input":"2021-11-08T03:16:09.563695Z","iopub.status.idle":"2021-11-08T03:16:15.798510Z","shell.execute_reply.started":"2021-11-08T03:16:09.563644Z","shell.execute_reply":"2021-11-08T03:16:15.797045Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(df2)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:15.800773Z","iopub.execute_input":"2021-11-08T03:16:15.801257Z","iopub.status.idle":"2021-11-08T03:16:15.810887Z","shell.execute_reply.started":"2021-11-08T03:16:15.801207Z","shell.execute_reply":"2021-11-08T03:16:15.809245Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def iqring(df, col):\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    df2 = df[~((df[col] < (Q1 - 1.5 * IQR)) |(df[col] > (Q3 + 1.5 * IQR)))]\n    return df2\ndf3 = iqring(df2, 'total_amount')\ndel df2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:15.812285Z","iopub.execute_input":"2021-11-08T03:16:15.812687Z","iopub.status.idle":"2021-11-08T03:16:22.821172Z","shell.execute_reply.started":"2021-11-08T03:16:15.812638Z","shell.execute_reply":"2021-11-08T03:16:22.819916Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def col_dummy(df, col):\n    cols_before = df.columns\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.codes\n    hood_series = pd.get_dummies(df[col],prefix=col)\n    df = pd.concat([df, hood_series], axis=1)\n    df = df.drop(columns=(col), axis=1)\n    cols_after = df.columns\n    new_cols = list(set(cols_after) - set(cols_before) )\n    del hood_series\n    gc.collect()\n    return df, new_cols","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:22.825280Z","iopub.execute_input":"2021-11-08T03:16:22.825571Z","iopub.status.idle":"2021-11-08T03:16:22.832510Z","shell.execute_reply.started":"2021-11-08T03:16:22.825540Z","shell.execute_reply":"2021-11-08T03:16:22.831758Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(df3)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:22.833907Z","iopub.execute_input":"2021-11-08T03:16:22.834144Z","iopub.status.idle":"2021-11-08T03:16:22.856530Z","shell.execute_reply.started":"2021-11-08T03:16:22.834116Z","shell.execute_reply":"2021-11-08T03:16:22.855821Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train_test(df, features):\n    np.random.seed(1)\n    shuffled_index = np.random.permutation(df.index)\n    df = df.reindex(index = shuffled_index)\n    split_loc = int(0.5*len(df))\n    # split\n    train = df.iloc[:split_loc].copy()\n    test = df.iloc[split_loc:].copy()\n    del df\n    gc.collect()\n    lr = linear_model.LinearRegression()\n    lr.fit(train[features], train[\"total_amount\"])\n    predictions = lr.predict(test[features])\n    mse = mean_squared_error(test[\"total_amount\"], predictions)\n    rmse = np.sqrt(mse)\n    \n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:22.857921Z","iopub.execute_input":"2021-11-08T03:16:22.858163Z","iopub.status.idle":"2021-11-08T03:16:22.875730Z","shell.execute_reply.started":"2021-11-08T03:16:22.858135Z","shell.execute_reply":"2021-11-08T03:16:22.873377Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train_test1(df, features):\n#     np.random.seed(1)\n#     shuffled_index = np.random.permutation(df.index)\n#     df = df.reindex(index = shuffled_index)\n#     split_loc = int(0.5*len(df))\n    # split\n    train = df[df['pickup_date'] < '2018-01-01'].copy()\n    test = df[df['pickup_date'] >= '2018-01-01'].copy()\n    del df\n    gc.collect()\n    lr = linear_model.LinearRegression()\n    lr.fit(train[features], train[\"total_amount\"])\n    predictions = lr.predict(test[features])\n    mse = mean_squared_error(test[\"total_amount\"], predictions)\n    rmse = np.sqrt(mse)\n    \n    return rmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:22.877885Z","iopub.execute_input":"2021-11-08T03:16:22.878751Z","iopub.status.idle":"2021-11-08T03:16:22.898657Z","shell.execute_reply.started":"2021-11-08T03:16:22.878709Z","shell.execute_reply":"2021-11-08T03:16:22.896486Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"######## PICK UP HERE","metadata":{"execution":{"iopub.status.busy":"2021-11-08T01:47:56.639402Z","iopub.execute_input":"2021-11-08T01:47:56.639683Z","iopub.status.idle":"2021-11-08T01:47:56.648886Z","shell.execute_reply.started":"2021-11-08T01:47:56.639637Z","shell.execute_reply":"2021-11-08T01:47:56.648083Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"features1 = ['trip_distance', 'trip_length']\n%memit rmse = train_test(df3, features1)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:22.901334Z","iopub.execute_input":"2021-11-08T03:16:22.901888Z","iopub.status.idle":"2021-11-08T03:16:46.258789Z","shell.execute_reply.started":"2021-11-08T03:16:22.901849Z","shell.execute_reply":"2021-11-08T03:16:46.256845Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"features1 = ['trip_distance', 'trip_length']\n%memit rmse = train_test1(df3, features1)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:46.261240Z","iopub.execute_input":"2021-11-08T03:16:46.261781Z","iopub.status.idle":"2021-11-08T03:16:52.425019Z","shell.execute_reply.started":"2021-11-08T03:16:46.261697Z","shell.execute_reply":"2021-11-08T03:16:52.423834Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"features1 = ['trip_distance', 'trip_length']\nrmse = train_test(df3, features1)\n# f1 2.6936316597764085\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:16:52.426881Z","iopub.execute_input":"2021-11-08T03:16:52.427247Z","iopub.status.idle":"2021-11-08T03:17:12.149787Z","shell.execute_reply.started":"2021-11-08T03:16:52.427197Z","shell.execute_reply":"2021-11-08T03:17:12.148711Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"features1 = ['trip_distance', 'trip_length']\nrmse = train_test1(df3, features1)\n# f1 2.6936316597764085\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:17:12.152734Z","iopub.execute_input":"2021-11-08T03:17:12.153036Z","iopub.status.idle":"2021-11-08T03:17:17.164309Z","shell.execute_reply.started":"2021-11-08T03:17:12.153002Z","shell.execute_reply":"2021-11-08T03:17:17.162773Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"features2 = ['trip_distance', 'trip_length', 'day_of_week', 'time_of_day']\nrmse = train_test(df3, features2)\n# f2 2.671386816892119\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:17:17.165780Z","iopub.execute_input":"2021-11-08T03:17:17.166051Z","iopub.status.idle":"2021-11-08T03:17:37.961994Z","shell.execute_reply.started":"2021-11-08T03:17:17.166021Z","shell.execute_reply":"2021-11-08T03:17:37.960942Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"features2 = ['trip_distance', 'trip_length', 'day_of_week', 'time_of_day']\nrmse = train_test1(df3, features2)\n# f2 3.48\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:17:37.963900Z","iopub.execute_input":"2021-11-08T03:17:37.964277Z","iopub.status.idle":"2021-11-08T03:17:43.952848Z","shell.execute_reply.started":"2021-11-08T03:17:37.964233Z","shell.execute_reply":"2021-11-08T03:17:43.951661Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"features5 = ['trip_distance', 'trip_length', 'day_of_week', 'time_of_day', 'tmin', 'tmax']\nrmse = train_test(df3, features5)\n# f5 2.6680336382474628\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:17:43.954203Z","iopub.execute_input":"2021-11-08T03:17:43.954498Z","iopub.status.idle":"2021-11-08T03:18:05.565079Z","shell.execute_reply.started":"2021-11-08T03:17:43.954465Z","shell.execute_reply":"2021-11-08T03:18:05.563799Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"%memit rmse = train_test(df3, features5)\n# #1318","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:18:05.566671Z","iopub.execute_input":"2021-11-08T03:18:05.566945Z","iopub.status.idle":"2021-11-08T03:18:28.325389Z","shell.execute_reply.started":"2021-11-08T03:18:05.566912Z","shell.execute_reply":"2021-11-08T03:18:28.324296Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"features5 = ['trip_distance', 'trip_length', 'day_of_week', 'time_of_day', 'tmin', 'tmax']\nrmse = train_test1(df3, features5)\n# f 5 3.4780707610134134\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:18:28.327241Z","iopub.execute_input":"2021-11-08T03:18:28.327892Z","iopub.status.idle":"2021-11-08T03:18:35.123646Z","shell.execute_reply.started":"2021-11-08T03:18:28.327837Z","shell.execute_reply":"2021-11-08T03:18:35.122674Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"%memit rmse = train_test1(df3, features5)\n#  1842.56 MiB","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:18:35.125197Z","iopub.execute_input":"2021-11-08T03:18:35.125496Z","iopub.status.idle":"2021-11-08T03:18:42.850698Z","shell.execute_reply.started":"2021-11-08T03:18:35.125462Z","shell.execute_reply":"2021-11-08T03:18:42.849577Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"df3, new_cols = col_dummy(df3, 'day_of_week')","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:18:53.999763Z","iopub.execute_input":"2021-11-08T03:18:54.000077Z","iopub.status.idle":"2021-11-08T03:19:02.368887Z","shell.execute_reply.started":"2021-11-08T03:18:54.000045Z","shell.execute_reply":"2021-11-08T03:19:02.367866Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"\nfeatures6 = ['trip_distance', 'trip_length', 'time_of_day', 'tmin', 'tmax'] + new_cols\nrmse = train_test(df3, features6)\n# f6 2.634177334931782\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:19:14.835460Z","iopub.execute_input":"2021-11-08T03:19:14.836040Z","iopub.status.idle":"2021-11-08T03:19:41.680584Z","shell.execute_reply.started":"2021-11-08T03:19:14.835996Z","shell.execute_reply":"2021-11-08T03:19:41.678944Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\nfeatures6 = ['trip_distance', 'trip_length', 'time_of_day', 'tmin', 'tmax'] + new_cols\nrmse = train_test1(df3, features6)\n# f6 3.4658308154700666\nrmse\n# 2.6013369689567116","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:20:24.500175Z","iopub.execute_input":"2021-11-08T03:20:24.500722Z","iopub.status.idle":"2021-11-08T03:20:35.117028Z","shell.execute_reply.started":"2021-11-08T03:20:24.500658Z","shell.execute_reply":"2021-11-08T03:20:35.115494Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df3, new_cols2 = col_dummy(df3, 'time_of_day')","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:20:55.904861Z","iopub.execute_input":"2021-11-08T03:20:55.905637Z","iopub.status.idle":"2021-11-08T03:21:06.793883Z","shell.execute_reply.started":"2021-11-08T03:20:55.905549Z","shell.execute_reply":"2021-11-08T03:21:06.792491Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\nfeatures7 = ['trip_distance', 'trip_length', 'tmin', 'tmax'] + new_cols + new_cols2\nrmse = train_test(df3, features7)\n# f7 2.5576173097913086\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:21:17.799733Z","iopub.execute_input":"2021-11-08T03:21:17.800023Z","iopub.status.idle":"2021-11-08T03:22:04.619781Z","shell.execute_reply.started":"2021-11-08T03:21:17.799994Z","shell.execute_reply":"2021-11-08T03:22:04.618577Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"\nfeatures7 = ['trip_distance', 'trip_length', 'tmin', 'tmax'] + new_cols + new_cols2\nrmse = train_test1(df3, features7)\n# f7 2.5576173097913086\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-11-08T03:22:22.945636Z","iopub.execute_input":"2021-11-08T03:22:22.946232Z","iopub.status.idle":"2021-11-08T03:22:47.632279Z","shell.execute_reply.started":"2021-11-08T03:22:22.946188Z","shell.execute_reply":"2021-11-08T03:22:47.631125Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"df3['total_amount'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train_test2(df, features):\n#     np.random.seed(1)\n#     shuffled_index = np.random.permutation(df.index)\n#     df = df.reindex(index = shuffled_index)\n#     split_loc = int(0.5*len(df))\n#     # split\n#     train = df.iloc[:split_loc].copy()\n#     test = df.iloc[split_loc:].copy()\n#     del df\n#     gc.collect()\n#     lr = linear_model.LinearRegression()\n#     lr.fit(train[features], train[\"total_amount\"])\n#     predictions = lr.predict(test[features])\n#     mse = mean_squared_error(test[\"total_amount\"], predictions)\n#     rmse = np.sqrt(mse)\n#     test['predicted'] = predictions\n    \n#     return rmse, test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features4 = ['trip_distance', 'trip_length', 'day_of_week', 'time_of_day']\n# rmse, test_df = train_test2(df3, features4)\n# test_df[['total_amount', 'predicted']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df3.groupby('pickup_date')['passenger_count'].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df3.to_csv('myfirst.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}