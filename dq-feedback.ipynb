{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport bs4\nfrom bs4 import BeautifulSoup\nimport requests\nimport urllib\nimport urllib.request\nfrom urllib.request import urlopen, Request\nimport codecs\nfrom textblob import TextBlob, Word, Blobber\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords')\nnltk.download('wordnet')\nimport string","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-24T07:34:07.640542Z","iopub.execute_input":"2021-11-24T07:34:07.640815Z","iopub.status.idle":"2021-11-24T07:34:09.504525Z","shell.execute_reply.started":"2021-11-24T07:34:07.640787Z","shell.execute_reply":"2021-11-24T07:34:09.503676Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"A while ago I've started writing a generic feedback post. I wanted to compile all my usual project feedback remarks into 1 list. So that everyone can have a read before/ after they publish their project and check if they still can do some work. That made me thinking: why don't include other peoples feedback as well? I've started wondering how many published projects received at least 1 reply on Dataquests forum? Currently (November 2021) that number sits at 1102 posts. At this stage this stopped looking like writing a post, and started looking like a data analysis project...\n\nStep 1:\nScrape all the necessary data (project posts)\n\nStep 2: \nClean data - we only want to analyze the posts with at least 1 reply\n\nStep 3: \nAnalyze data","metadata":{}},{"cell_type":"markdown","source":"Dataquest offers online, project-based data science courses focused on data analysis using R and Python. Part of every course is a hands on project to practice your skills in real world applications. After finishing your project, the platform encourages you to publish it on their forum to gather feedback. \n\nI've benefited a lot from various people sharing their insights on my work. As I've progressed, I've started giving back and showing other people what I would have done differently in their projects. This led me to this project. In this notebook I'll gather the data from all the feedback posts, clean it and analyze it. That should give us an insight on the most common remarks regarding our published projects. After the analysis step, we'll try to train a few machine learning models on recognizing the feedback content and classifying it.","metadata":{}},{"cell_type":"code","source":"# step 1:\nurl = \"https://community.dataquest.io/c/share/guided-project/55\"\nhtml = urlopen(url)\nsoup = BeautifulSoup(html, 'lxml')\nlist_all = soup.find_all(\"a\", class_=\"title raw-link raw-topic-link\")\nlen(list_all)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:34:10.200445Z","iopub.execute_input":"2021-11-24T07:34:10.200779Z","iopub.status.idle":"2021-11-24T07:34:10.529707Z","shell.execute_reply.started":"2021-11-24T07:34:10.200744Z","shell.execute_reply":"2021-11-24T07:34:10.528891Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"We'll start with scraping the dataquests forum page for publishing projects, every thread on that page represents a different project. We're interested in the content of those threads. \n\nTrying to scrape this website leads to first problem: the website displays only 30 threads. I've tried diffrent path options and couldn't find a way to target the next 30 posts. So I came up with a brutal and simple solution:\n* manually scroll down to the bottom of the website (so that all posts are displayed)\n* save the website to a file\n* load the file to the notebook and keep on scraping","metadata":{}},{"cell_type":"code","source":"# # lets scrape the html content from the saved file:\nfile = codecs.open(\"../input/dq-projects/projects.html\", \"r\", \"utf-8\")\nparser = BeautifulSoup(file, 'html.parser')\nlist_all = parser.find_all('tr')\nseries_4_df = pd.Series(list_all)\n# create a dataframe with values(title, link, etc.) extracted from the html file:\ndf = pd.DataFrame(series_4_df, columns=['col1'])\ndf['col1'] = df['col1'].astype(str)\ndf = df.iloc[1:,:]\n\ndf['title'] = df['col1'].str.extract('<span dir=\"ltr\">(.*?)</span>')\ndf['link'] = df['col1'].str.extract('href=(.*?)level=\"2\"')\ndf['replies'] = df['col1'].str.extract(\"This topic has (.*?) re\").astype(int)\n\n# filter out posts with more than 100 replies (1 post that's a general thread) and posts without feedback:\ndf = df.drop(columns='col1')\ndf = df[df['replies']>0]\ndf = df[df['replies']<100]\ndf = df.reset_index()\n\ndf['link2'] = df['link'].str.extract('\\\"(.*?)\\\"')\ndf = df.drop(columns=['index', 'link'])\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:34:12.955683Z","iopub.execute_input":"2021-11-24T07:34:12.955978Z","iopub.status.idle":"2021-11-24T07:34:19.263517Z","shell.execute_reply.started":"2021-11-24T07:34:12.955945Z","shell.execute_reply":"2021-11-24T07:34:19.262668Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# create a function for scraping the actual posts website:\ndef get_reply(one_link):\n    response = requests.get(one_link)\n    content = response.content\n    parser = BeautifulSoup(content, 'html.parser')\n    tag_numbers = parser.find_all(\"div\", class_=\"post\")\n    # we're only going to scrape the content of the first reply (that's usually the feedback)\n    feedback = tag_numbers[1].text\n    return feedback\n\n# create a test dataframe to test scraping:\ndf_test = df[:2].copy()\n\n# we'll use a loop on all the elements of pd.Series (fastern than using 'apply')\nfeedback_list = []\nfor el in df_test['link2']:\n    feedback_list.append(get_reply(el))\ndf_test['feedback'] = feedback_list\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:17:53.748512Z","iopub.execute_input":"2021-11-24T07:17:53.748703Z","iopub.status.idle":"2021-11-24T07:17:54.868468Z","shell.execute_reply.started":"2021-11-24T07:17:53.748676Z","shell.execute_reply":"2021-11-24T07:17:54.867845Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"### It works! Time to try it out on a bigger fish:","metadata":{}},{"cell_type":"code","source":"feedback_list = []\nfor el in df['link2']:\n    feedback_list.append(get_reply(el))\ndf['feedback'] = feedback_list\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:17:54.869311Z","iopub.execute_input":"2021-11-24T07:17:54.869493Z","iopub.status.idle":"2021-11-24T07:27:55.460944Z","shell.execute_reply.started":"2021-11-24T07:17:54.869471Z","shell.execute_reply":"2021-11-24T07:27:55.459598Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# thanks to rohithramesh1991 for the below function:\n# source: https://github.com/rohithramesh1991/Text-Preprocessing/blob/master/Text%20Preprocessing_codes.py\ndef text_process(text):\n    '''\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Return the cleaned text as a list of words\n    4. Remove words\n    '''\n    stemmer = WordNetLemmatizer()\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join([i for i in nopunc if not i.isdigit()])\n    nopunc =  [word.lower() for word in nopunc.split() if word not in stopwords.words('english')]\n    return [stemmer.lemmatize(word) for word in nopunc]\n\ndf['cleaned'] = df['feedback'].apply(text_process)\nstop = stopwords.words('english')\ndf['no_stop'] = df['feedback'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf['no_stop'] = df['no_stop'].str.replace('[^\\w\\s]','', regex=True)\n\nfreq = pd.Series(' '.join(df['no_stop']).split()).value_counts()[:15]\nfreq = list(freq.index) + ['congratulation', 'happy', 'learning', 'guided', 'community', 'luck']\ndf['top10_removed'] = df['no_stop'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n# df['top10_removed'] = df['top10_removed'].apply(text_process)\ndf = df.drop(columns='no_stop')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:27:55.462625Z","iopub.execute_input":"2021-11-24T07:27:55.462895Z","iopub.status.idle":"2021-11-24T07:28:09.271310Z","shell.execute_reply.started":"2021-11-24T07:27:55.462856Z","shell.execute_reply":"2021-11-24T07:28:09.270104Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"df.to_csv('dq.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:09.272467Z","iopub.execute_input":"2021-11-24T07:28:09.272642Z","iopub.status.idle":"2021-11-24T07:28:09.331248Z","shell.execute_reply.started":"2021-11-24T07:28:09.272620Z","shell.execute_reply":"2021-11-24T07:28:09.330337Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"## Most popular words:","metadata":{}},{"cell_type":"code","source":"df['temp_list'] = df['top10_removed'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:09.333386Z","iopub.execute_input":"2021-11-24T07:28:09.333707Z","iopub.status.idle":"2021-11-24T07:28:09.384456Z","shell.execute_reply.started":"2021-11-24T07:28:09.333678Z","shell.execute_reply":"2021-11-24T07:28:09.383641Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"## N-grams\nN-grams are continuous sequences of words or symbols or tokens in a document. They can be defined as the neighbouring sequences of items in a document.In our example we'll be looking at trigrams(3 words) and n-grams of 4 words. We'll combine all the feedback posts we've scraped and analyze, which n-grams were the most common.","metadata":{}},{"cell_type":"code","source":"def find_ngrams(input_list, n):\n    return zip(*(input_list[i:] for i in range(n)))\n\ndef pop_trigram(words):\n    trigrams = find_ngrams(words, 3)\n    counts_tri = Counter(trigrams)\n    return counts_tri.most_common() \n\ndef pop_4gram(words):\n    quadgrams = find_ngrams(words, 4)\n    counts_4 = Counter(quadgrams)\n    return counts_4.most_common() \n\nexclude = ['happy', 'congratulation', 'learning', 'community', 'feedback', 'project', 'guided', 'job', 'great', 'example', \n           'sharing', 'suggestion', 'share', 'download', 'topic', 'everything', 'nice', 'well', 'done', 'look', 'file', 'might']\ninclude = ['use', 'consider', 'should', 'make', 'get', 'give', 'should']\n\ntrigrams_cnt = pop_trigram(df['top10_removed'].str.split(' ').sum())\ntrigrams = pd.DataFrame(trigrams_cnt, columns=['trigram','count'])\ntrigrams['word1'], trigrams['word2'], trigrams['word3'] = trigrams['trigram'].str[0], trigrams['trigram'].str[1], trigrams['trigram'].str[2]\ntrigrams['Bool'] = trigrams['word1'].isin(exclude) | trigrams['word2'].isin(exclude) | trigrams['word3'].isin(exclude)\ntrigrams = trigrams[trigrams['Bool']==False]\ntrigrams['Bool2'] = trigrams['word1'].isin(include) \ntrigrams = trigrams[trigrams['Bool2']==True]\ntrigrams[['trigram', 'count']][:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:35:37.651794Z","iopub.execute_input":"2021-11-24T07:35:37.652569Z","iopub.status.idle":"2021-11-24T07:35:37.794190Z","shell.execute_reply.started":"2021-11-24T07:35:37.652525Z","shell.execute_reply":"2021-11-24T07:35:37.792811Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"fourgrams_cnt = pop_4gram(df['top10_removed'].str.split(' ').sum())\nf4grams = pd.DataFrame(fourgrams_cnt,columns=['qgram','count'])\nf4grams['word1'], f4grams['word2'], f4grams['word3'], f4grams['word4'] = f4grams['qgram'].str[0], f4grams['qgram'].str[1], f4grams['qgram'].str[2], f4grams['qgram'].str[3]\nf4grams['Bool'] = f4grams['word1'].isin(exclude) | f4grams['word2'].isin(exclude)| f4grams['word3'].isin(exclude)| f4grams['word4'].isin(exclude)\nf4grams = f4grams[f4grams['Bool']==False]\nf4grams['Bool2'] = f4grams['word1'].isin(include) \nf4grams = f4grams[f4grams['Bool2']==True]\nf4grams[['qgram', 'count']][:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:10.011748Z","iopub.execute_input":"2021-11-24T07:28:10.011950Z","iopub.status.idle":"2021-11-24T07:28:10.710273Z","shell.execute_reply.started":"2021-11-24T07:28:10.011922Z","shell.execute_reply":"2021-11-24T07:28:10.709170Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"stopwords1 = list(STOPWORDS) \nmore_stopwords = ['also', 'it', 'thank', 'think', 'one', 'thanks']\nstopwords2 = stopwords1+more_stopwords\n\ntext =  str(df['top10_removed'].sum())\ntext = text.replace(\"'\",\"\")\nwordcloud = WordCloud(width=1600, height=800, stopwords=stopwords2)\nwordcloud.generate(text)\n# Open a plot of the generated image.\n\nplt.figure( figsize=(20,10), facecolor='k')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:10.711521Z","iopub.execute_input":"2021-11-24T07:28:10.711761Z","iopub.status.idle":"2021-11-24T07:28:14.419698Z","shell.execute_reply.started":"2021-11-24T07:28:10.711734Z","shell.execute_reply":"2021-11-24T07:28:14.418465Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"# Machine learning","metadata":{}},{"cell_type":"code","source":"learn_tri = trigrams[['trigram', 'count']]\n# manualy mark whether the trigram is helpful or not:\nlist20 = [0,1,1,1,1,1,1,1,0,1,0,1,1,1,1,1,0,1,0,1]\nlist20_40 = [1,0,1,1,0,1,0,0,0,1,1,0,1,1,1,1,1,1,1,1]\nlist40_60 = [1,0,1,0,1,1,1,0,1,0,0,1,1,0,0,0,0,0,1,1]\nlist60_80 = [1,0,0,1,1,0,1,0,0,1,0,1,0,0,1,1,0,1,1,0]\n\ntrain = learn_tri[:80].copy()\ntest = learn_tri[80:].copy()\n\ntrain['trigram'] = train['trigram'].str.join(',').str.lower().str.replace(',',' ')\ntest['trigram'] = test['trigram'].str.join(',').str.lower().str.replace(',',' ')\n\nlist_helpful = list20+list20_40+list40_60+list60_80\ntrain['helpful'] = list_helpful\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:14.421022Z","iopub.execute_input":"2021-11-24T07:28:14.421247Z","iopub.status.idle":"2021-11-24T07:28:14.452052Z","shell.execute_reply.started":"2021-11-24T07:28:14.421217Z","shell.execute_reply":"2021-11-24T07:28:14.451285Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\n# preprocess data:\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(train['trigram'].values)\nX = tokenizer.texts_to_sequences(train['trigram'].values)\nX = pad_sequences(X)\nY = tokenizer.texts_to_sequences(test['trigram'].values)\nY = pad_sequences(Y)\n\n# split dataset:\nX_train = X\ny_train = train['helpful']\nX_test = Y\n\ndef fit_model(model):\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    return predictions\n\n# create models:\nmodel1 = KNeighborsClassifier(n_neighbors = 1)\nmodel2 = RandomForestClassifier()\nmodel3 = GaussianNB()\nmodel4 = ExtraTreesClassifier(bootstrap=False, criterion='entropy', max_features=0.55, min_samples_leaf=8, min_samples_split=4, n_estimators=100) # Optimized using TPOT\nmodel5 = MLPClassifier(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (10,10,10),\n                            learning_rate = \"constant\", max_iter = 2000, random_state = 1)\n\n# fit models:\ntest['KNeighbors'] = fit_model(model1)\ntest['RandomForest'] = fit_model(model2)\ntest['GaussianNB'] = fit_model(model3)\ntest['ExtraTree'] = fit_model(model4)\ntest['MLP'] = fit_model(model5)\ntest","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:14.453313Z","iopub.execute_input":"2021-11-24T07:28:14.454032Z","iopub.status.idle":"2021-11-24T07:28:14.758779Z","shell.execute_reply.started":"2021-11-24T07:28:14.454004Z","shell.execute_reply":"2021-11-24T07:28:14.758355Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"### Check value counts:","metadata":{}},{"cell_type":"code","source":"for col in test.columns[2:]:\n    print(test[col].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:14.759599Z","iopub.execute_input":"2021-11-24T07:28:14.760496Z","iopub.status.idle":"2021-11-24T07:28:14.768005Z","shell.execute_reply.started":"2021-11-24T07:28:14.760468Z","shell.execute_reply":"2021-11-24T07:28:14.767335Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"All of the models failed at any classification attempt. It's worth noting though that the MLP is the only model which hasn't assigned only 1 value to an overwhelming majority of rows. We'll try improving that model.","metadata":{}},{"cell_type":"code","source":"test = test[['trigram', 'count', 'MLP']].copy()\ntest[:20]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:14.768850Z","iopub.execute_input":"2021-11-24T07:28:14.769009Z","iopub.status.idle":"2021-11-24T07:28:14.788311Z","shell.execute_reply.started":"2021-11-24T07:28:14.768989Z","shell.execute_reply":"2021-11-24T07:28:14.787916Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"# Apply gridsearch for hyperparameter tuning\n(btw this was supposed to be an easy EDA project to catch a break from ML)","metadata":{}},{"cell_type":"code","source":"mlp = MLPClassifier(max_iter=3000, random_state=0)\n\nparameter_space = {\n    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.0001, 0.05],\n    'learning_rate': ['constant','adaptive']}\n\nclf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\nclf.fit(X_train, y_train)\nprint(clf.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:14.789052Z","iopub.execute_input":"2021-11-24T07:28:14.789797Z","iopub.status.idle":"2021-11-24T07:28:32.382905Z","shell.execute_reply.started":"2021-11-24T07:28:14.789737Z","shell.execute_reply":"2021-11-24T07:28:32.381905Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"## Apply best parameters: ","metadata":{}},{"cell_type":"code","source":"model6 = MLPClassifier(activation = \"tanh\", alpha = 0.0001, hidden_layer_sizes = (50, 50, 50),\n                            learning_rate = \"constant\", max_iter = 2000, random_state = 1, solver='sgd')\n\ntest['MLP_v2'] = fit_model(model5)\n\n# check if there's any diffrence in results:\ntest[test['MLP_v2'] != test['MLP']]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:32.383941Z","iopub.execute_input":"2021-11-24T07:28:32.384128Z","iopub.status.idle":"2021-11-24T07:28:32.431120Z","shell.execute_reply.started":"2021-11-24T07:28:32.384105Z","shell.execute_reply":"2021-11-24T07:28:32.430332Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"# Kmeans clustering","metadata":{}},{"cell_type":"code","source":"trigrams['trigram'] = trigrams['trigram'].str.join(',').str.lower().str.replace(',',' ')\n# vectorize:\ntfidfconverter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \nX = tfidfconverter.fit_transform(trigrams['trigram']).toarray()\n# fit and cluster:\nKmean = KMeans(n_clusters=3)\nKmean.fit(X)\ntrigrams['label'] = Kmean.labels_","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:32.432216Z","iopub.execute_input":"2021-11-24T07:28:32.432464Z","iopub.status.idle":"2021-11-24T07:28:33.388817Z","shell.execute_reply.started":"2021-11-24T07:28:32.432413Z","shell.execute_reply":"2021-11-24T07:28:33.388288Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"trigrams[trigrams['label'] == 0]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:33.389539Z","iopub.execute_input":"2021-11-24T07:28:33.389704Z","iopub.status.idle":"2021-11-24T07:28:33.431315Z","shell.execute_reply.started":"2021-11-24T07:28:33.389681Z","shell.execute_reply":"2021-11-24T07:28:33.430475Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"trigrams[trigrams['label'] == 1]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:33.432389Z","iopub.execute_input":"2021-11-24T07:28:33.432661Z","iopub.status.idle":"2021-11-24T07:28:33.457547Z","shell.execute_reply.started":"2021-11-24T07:28:33.432629Z","shell.execute_reply":"2021-11-24T07:28:33.456986Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"trigrams[trigrams['label'] == 2]","metadata":{"execution":{"iopub.status.busy":"2021-11-24T07:28:33.460611Z","iopub.execute_input":"2021-11-24T07:28:33.462021Z","iopub.status.idle":"2021-11-24T07:28:33.481713Z","shell.execute_reply.started":"2021-11-24T07:28:33.461991Z","shell.execute_reply":"2021-11-24T07:28:33.481224Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"# Conclusions:","metadata":{}},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]}]}